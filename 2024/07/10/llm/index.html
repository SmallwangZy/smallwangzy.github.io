<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=5"><title>Student Wang Personal Blog</title><meta name="author" content="Wang Zhengyang"><link rel="shortcut icon" href="/img/favicon.png"><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.13.0/css/all.min.css"><meta name="generator" content="Hexo 7.3.0"></head><body><header id="page_header"><div class="header_wrap"><div id="blog_name"><a class="blog_title" id="site-name" href="/">Student Wang Personal Blog</a></div><button class="menus_icon"><div class="navicon"></div></button><ul class="menus_items"><li class="menus_item"><a class="site-page" href="/#Publications"> Publications</a></li><li class="menus_item"><a class="site-page" href="/"> About</a></li><li class="menus_item"><a class="site-page" target="_blank" rel="noopener" href="https://phower.me"> Blog</a></li></ul></div></header><main id="page_main"><div class="side-card sticky"><div class="card-wrap" itemscope itemtype="http://schema.org/Person"><div class="author-avatar"><img class="avatar-img" src="/img/avatar.jpg" onerror="this.onerror=null;this.src='/img/idphoto.jpg'" alt="avatar"></div><div class="author-discrip"><h3>Wang Zhengyang</h3><p class="author-bio">A college student from Chinese University of Geoscience majoring in the Electronic Information Engineering.</p></div><div class="author-links"><button class="btn m-social-links">Links</button><ul class="social-icons"><li><a class="social-icon" href="/" target="_blank"><i class="fab fa-twitter" aria-hidden="true"></i></a></li><li><a class="social-icon" href="/" target="_blank"><i class="fab fa-facebook-square" aria-hidden="true"></i></a></li><li><a class="social-icon" href="/" target="_blank"><i class="fab fa-github" aria-hidden="true"></i></a></li><li><a class="social-icon" href="/" target="_blank"><i class="fab fa-stack-overflow" aria-hidden="true"></i></a></li><li><a class="social-icon" href="/" target="_blank"><i class="fab fa-linkedin" aria-hidden="true"></i></a></li><li><a class="social-icon" href="/" target="_blank"><i class="fab fa-weibo" aria-hidden="true"></i></a></li><li><a class="social-icon" href="/" target="_blank"><i class="fab fa-weixin" aria-hidden="true"></i></a></li><li><a class="social-icon" href="/" target="_blank"><i class="fab fa-qq" aria-hidden="true"></i></a></li><li><a class="social-icon" href="/" target="_blank"><i class="fas fa-envelope" aria-hidden="true"></i></a></li><li><a class="social-icon" href="/" target="_blank"><i class="fas fa-rss" aria-hidden="true"></i></a></li></ul><ul class="social-links"><li><a class="e-social-link" href="/" target="_blank"><i class="fas fa-graduation-cap" aria-hidden="true"></i><span>Google Scholar</span></a></li><li><a class="e-social-link" href="/" target="_blank"><i class="fab fa-orcid" aria-hidden="true"></i><span>ORCID</span></a></li></ul></div><a class="cv-links" href="/attaches/CV.pdf" target="_blank"><i class="fas fa-file-pdf" aria-hidden="true"><span>My Detail CV.</span></i></a></div></div><div class="page" itemscope itemtype="http://schema.org/CreativeWork"><h2 class="page-title"></h2><article><hr>
<h2 id="Large-Language-Model"><a href="#Large-Language-Model" class="headerlink" title="Large Language Model"></a>Large Language Model</h2><h1 id="MM-LLM"><a href="#MM-LLM" class="headerlink" title="MM-LLM"></a>MM-LLM</h1><p>Multimodal LLM pays attention to the three different input types: </p>
<ul>
<li>text: text encoder.</li>
<li>image: image encoder.</li>
<li>voice</li>
<li>Video: a series of continuous frames</li>
</ul>
<p>It is unrealistic to use three different models for three different types respectively. Therefore we need an encoder to preprocess them and input them into the LLM.</p>
<img src="https://img-blog.csdnimg.cn/direct/6aa4a73540814e088e0d45c436f30b21.png" style="zoom: 50%;" />

<h1 id="Modality-Encoder"><a href="#Modality-Encoder" class="headerlink" title="Modality Encoder"></a>Modality Encoder</h1><p>For text, the computer can not realize the natural language, the language text will be tokenized and embedded:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">graph LR</span><br><span class="line">text--tokenizer--&gt;tokens--embedding--&gt;vectors</span><br></pre></td></tr></table></figure>

<p>Common tokenizer: BPE, world piece, ……. </p>
<p>The tokenizer can be divided into three types[^3]:</p>
<ul>
<li>Word-based method. <code>[&#39;I&#39;,&#39; am&#39;,&#39; a&#39;,&#39; student&#39;]</code></li>
<li>Character-based method. <code>[&#39;I&#39;,&#39;a&#39;,&#39;m&#39;,&#39;a&#39;,&#39;s&#39;,&#39;t&#39;,&#39;u&#39;,&#39;d&#39;,&#39;e&#39;,&#39;n&#39;,&#39;t&#39;]</code></li>
<li>Subword-based method.</li>
</ul>
<p>What does the tokenizer do actually? it transforms the text of a series of words into a computer-recognizable data structure. For example:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Using the word-based tokenizer:</span></span><br><span class="line">Input = <span class="string">&quot;I am a student&quot;</span></span><br><span class="line">&gt;&gt; [<span class="string">&#x27;I&#x27;</span>,<span class="string">&#x27; am&#x27;</span>,<span class="string">&#x27; a&#x27;</span>,<span class="string">&#x27; student&#x27;</span>]</span><br></pre></td></tr></table></figure>

<hr>
<p>For images, it is known that CLIP, Vision-transformer, and so on. Vision-transformer can be the image encoder, it divides the image into small patches, flattens each patch to a vector then puts it into a linear layer to transform the dimension, the option is called embedding[^3].</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">graph LR</span><br><span class="line">image--&gt;patches--&gt;vectors</span><br></pre></td></tr></table></figure>

<p>In the vision-transformer (for example):</p>
<ul>
<li><p>a image is composed of $256(&#x3D;1024&#x2F;4)$ patchs (patch size &#x3D;$4*4$)</p>
</li>
<li><p>each patch: $4<em>4</em>3$ (RGB channel), flatten output: $1<em>48$, linear output: $1</em>768$ (a matrix $768*48$)</p>
</li>
<li><p>a image can be represented by a $768*256$ vectors.</p>
</li>
</ul>
<h1 id="Modal-Alignment"><a href="#Modal-Alignment" class="headerlink" title="Modal Alignment"></a>Modal Alignment</h1><p>Although the text vector and image vector are both the vectors that can be recognized by the backbone (transformer).</p>
<p>However, the two vectors may represent different meanings. </p>
<p>The <strong>projector</strong> aims to align the two modal vectors.</p>
<h2 id="Input-Projector"><a href="#Input-Projector" class="headerlink" title="Input Projector"></a>Input Projector</h2><h2 id="Output-Projector"><a href="#Output-Projector" class="headerlink" title="Output Projector"></a>Output Projector</h2><h1 id="LLM-Training"><a href="#LLM-Training" class="headerlink" title="LLM Training"></a>LLM Training</h1><p>The training of LLM can be divided into two stages:</p>
<ul>
<li>Pretrain: using the specific datasets to train the input and output projector. (align the different modalities)</li>
<li>Fine-tuning:</li>
</ul>
<h1 id="RAG"><a href="#RAG" class="headerlink" title="RAG"></a>RAG</h1><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p>[^1]:<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/697626191">GPT-4o技术粗粗粗解 - 知乎</a><br>[^2]:<a target="_blank" rel="noopener" href="https://blog.csdn.net/imwaters/article/details/136370035">【论文综述+多模态】腾讯发布的多模态大语言模型（MM-LLM）综述(2024.02)_多模态大语言模型综述-CSDN博客</a><br>[^3]:<a target="_blank" rel="noopener" href="https://segmentfault.com/a/1190000044778456">人工智能 - 一站式解读多模态——Transformer、Embedding、Vision-transforemer主流模型与通用任务实战（上） - 百度飞桨 - SegmentFault 思否</a></p>
</article></div></main><div class="nav-wrap"><div class="nav"><button class="site-nav"><div class="navicon"></div></button><ul class="nav_items"><li class="nav_item"><a class="nav-page" href="/#Publications"> Publications</a></li><li class="nav_item"><a class="nav-page" href="/"> About</a></li><li class="nav_item"><a class="nav-page" target="_blank" rel="noopener" href="https://phower.me"> Blog</a></li></ul></div><div class="cd-top"><i class="fa fa-arrow-up" aria-hidden="true"></i></div></div><footer id="page_footer"><div class="footer_wrap"><div class="copyright">&copy;2020 - 2024 by Wang Zhengyang</div><div class="theme-info">Powered by <a target="_blank" href="https://hexo.io" rel="nofollow noopener">Hexo</a> & <a target="_blank" href="https://github.com/PhosphorW/hexo-theme-academia" rel="nofollow noopener">Academia Theme</a></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery-pjax@latest/jquery.pjax.min.js"></script><script src="/js/main.js"></script></body></html>